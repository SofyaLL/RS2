{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification. Linear models and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementing Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you need to implement Logistic Regression with l2 regularization using gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression loss:\n",
    "$$ L(w) = \\dfrac{1}{N}\\sum_{i=1}^N \\log(1 + e^{-\\langle w, x_i \\rangle y_i}) + \\frac{1}{2C} \\lVert w \\rVert^2  \\to \\min_w$$\n",
    "$$\\langle w, x_i \\rangle = \\sum_{j=1}^n w_{j}x_{ij} + w_{0},$$ $$ y_{i} \\in \\{-1, 1\\}$$ where $n$ is the number of features and $N$ is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent step:\n",
    "$$w^{(t+1)} := w^{(t)} + \\dfrac{\\eta}{N}\\sum_{i=1}^N y_ix_i \\Big(1 - \\dfrac{1}{1 + exp(-\\langle w^{(t)}, x_i \\rangle y_i)}\\Big) - \\eta \\frac{1}{C} w,$$\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2 points)** Implement the algorithm and use it to classify the digits (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) into \"even\" and \"odd\" categories. \"Even\" and \"Odd\" classes  should correspond to {-1, 1} labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopping criteria: either the number of iterations exceeds *max_iter* or $||w^{(t+1)} - w^{(t)}||_2 < tol$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import NotFittedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### NEW ONE \n",
    "class CustomLogisticRegression:\n",
    "    _estimator_type = \"classifier\"\n",
    "    \n",
    "    def __init__(self, eta=0.001, max_iter=1000, C=1.0, tol=1e-5, random_state=42, zero_init=False):\n",
    "        \"\"\"Logistic Regression classifier.\n",
    "        \n",
    "        Args:\n",
    "            eta: float, default=0.001\n",
    "                Learning rate.\n",
    "            max_iter: int, default=1000\n",
    "                Maximum number of iterations taken for the solvers to converge.\n",
    "            C: float, default=1.0\n",
    "                Inverse of regularization strength; must be a positive float.\n",
    "                Smaller values specify stronger regularization.\n",
    "            tol: float, default=1e-5\n",
    "                Tolerance for stopping criteria.\n",
    "            random_state: int, default=42\n",
    "                Random state.\n",
    "            zero_init: bool, default=False\n",
    "                Zero weight initialization.\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "        self.random_state = np.random.RandomState(seed=random_state)\n",
    "        self.zero_init = zero_init\n",
    "         \n",
    "    def get_sigmoid(self, X, weights):\n",
    "        return 1 / (1 + np.exp(-X.dot(weights.T)))\n",
    "\n",
    "\n",
    "    def get_loss(self, X, weights, y):\n",
    "        \"\"\"Calculate the loss.\"\"\"\n",
    "        mult_weights = sum(X.dot(weights.T))\n",
    "        return np.average(np.log(1+np.exp(-mult_weights*y))) + 0.5 / self.C * np.linalg.norm(weights)\n",
    "                    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Target vector.        \n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "        X_ext = np.hstack([np.ones((X.shape[0], 1)), X]) # a constant feature is included to handle intercept\n",
    "        num_features = X_ext.shape[1]\n",
    "        if self.zero_init:\n",
    "            self.weights_ = np.zeros(num_features) \n",
    "        else:\n",
    "            weight_threshold = 1.0 / (2 * num_features)\n",
    "            self.weights_ = self.random_state.uniform(low=-weight_threshold,\n",
    "                                                      high=weight_threshold, size=num_features)\n",
    "\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            loss_history.append(self.get_loss(X_ext, self.weights_, y))\n",
    "            \n",
    "            arr = (1-1/(1+np.exp(-sum(X_ext.dot(self.weights_.T))*y)))\n",
    "            arr_2 = np.average(y[:,None]*X_ext*arr[:,None],axis=0)\n",
    "            delta = -arr_2 + 1/self.C * self.weights_\n",
    "            self.weights_ -= self.eta * delta\n",
    "                    \n",
    "#                     # код тянучки\n",
    "#             delta = y[i] * X_ext[i] * (1 -1 / (1 + np.exp(-y[i] * (np.dot(Xext[i], self.weights[i]))))) \n",
    "#             self.weights_ -= self.eta * delta \n",
    "                    \n",
    "            if (i > self.max_iter) or np.linalg.norm(delta) < self.tol:   \n",
    "                print('!!!!! tol')\n",
    "                break\n",
    "        return loss_history, self.weights_\n",
    "     \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict positive class probabilities.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing positive class probabilities.\n",
    "        \"\"\"\n",
    "        X_ext = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        if hasattr(self, 'weights_'):\n",
    "            return self.get_sigmoid(X_ext, self.weights_)\n",
    "        else: \n",
    "            raise NotFittedError(\"CustomLogisticRegression instance is not fitted yet\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing predicted class labels.\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return [1 if i>0.5 else -1 for i in proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 21 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = datasets.load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "_, axes = plt.subplots(nrows=3, ncols=7, figsize=(10, 5))\n",
    "for ax, image, label in zip(axes.flatten(), X, y):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image.reshape((8, 8)), cmap=plt.cm.gray_r if label % 2 else plt.cm.afmhot_r)\n",
    "    ax.set_title(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "y_train = (y_train % 2) * 2 - 1\n",
    "y_test = (y_test % 2) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.unique(y_train) == [-1, 1]).all()\n",
    "assert (np.unique(y_test) == [-1, 1]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# изменила\n",
    "def fit_evaluate(clf, X_train, y_train, X_test, y_test):\n",
    "    loss, weights = clf.fit(X_train, y_train)\n",
    "    disp = metrics.plot_confusion_matrix(clf, X_test, y_test, normalize='true')\n",
    "    disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics.accuracy_score(y_pred=clf.predict(X_train), y_true=y_train), \\\n",
    "           metrics.accuracy_score(y_pred=clf.predict(X_test), y_true=y_test), \\\n",
    "           loss, weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = CustomLogisticRegression(max_iter=1, zero_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(lr_clf.get_sigmoid(np.array([[0.5, 0, 1.0], [0.3, 1.3, 1.0]]), np.array([0.5, -0.5, 0.1])),\n",
    "                   np.array([0.58662, 0.40131]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(lr_clf.weights_, np.array([ 3.1000e-06,  0.0000e+00,  4.1800e-05,  5.4770e-04,  2.2130e-04,\n",
    "        4.8750e-04,  1.3577e-03,  5.9780e-04,  5.6400e-05, -7.0000e-07,\n",
    "        1.6910e-04,  2.5190e-04, -4.3700e-04,  3.6190e-04,  1.0049e-03,\n",
    "        4.2280e-04,  2.5700e-05,  3.0000e-07, -1.1500e-05, -7.2440e-04,\n",
    "       -2.6200e-04,  8.7540e-04,  4.1540e-04, -8.4200e-05, -5.2000e-06,\n",
    "        0.0000e+00, -2.2160e-04, -5.7130e-04,  9.8570e-04,  1.3507e-03,\n",
    "        5.0210e-04, -1.7050e-04, -1.0000e-06,  0.0000e+00, -6.7810e-04,\n",
    "       -1.0515e-03, -4.4500e-05,  3.7160e-04,  4.2100e-04, -8.1800e-05,\n",
    "        0.0000e+00, -5.2000e-06, -5.3410e-04, -2.0393e-03, -8.4310e-04,\n",
    "        1.0400e-04, -1.2390e-04, -1.7880e-04, -1.3200e-05, -4.5000e-06,\n",
    "       -9.4300e-05, -1.1127e-03, -5.0900e-04, -2.1850e-04, -5.6050e-04,\n",
    "       -3.9560e-04, -1.7700e-05, -3.0000e-07,  2.6800e-05,  6.3920e-04,\n",
    "        1.8090e-04, -7.3660e-04, -5.3930e-04, -3.7060e-04, -2.8200e-05]), atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wl/27ymcjln6px3tcfww0c_jg5c0000gn/T/ipykernel_16062/563954446.py:37: RuntimeWarning: overflow encountered in exp\n",
      "  return np.average(np.log(1+np.exp(-mult_weights*y))) + 0.5 / self.C * np.linalg.norm(weights)\n",
      "/var/folders/wl/27ymcjln6px3tcfww0c_jg5c0000gn/T/ipykernel_16062/563954446.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  arr = (1-1/(1+np.exp(-sum(X_ext.dot(self.weights_.T))*y)))\n",
      "/Users/sonya/Library/Python/3.8/lib/python/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc, test_acc,loss, weights = fit_evaluate(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8364648573416841, 0.8694444444444445)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wl/27ymcjln6px3tcfww0c_jg5c0000gn/T/ipykernel_16017/4215115562.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert min(train_acc, test_acc) > 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Visualize the loss history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[109.27675949704542,\n",
       " inf,\n",
       " 114.08109538603016,\n",
       " inf,\n",
       " 118.87693792693003,\n",
       " inf,\n",
       " 123.66430306758906,\n",
       " inf,\n",
       " 128.44302772570725,\n",
       " inf,\n",
       " 133.21288514884534,\n",
       " inf,\n",
       " 137.97368147963604,\n",
       " inf,\n",
       " 142.72528305632173,\n",
       " inf,\n",
       " 147.46760718781508,\n",
       " inf,\n",
       " 152.20060629142793,\n",
       " inf,\n",
       " 156.92425536846667,\n",
       " inf,\n",
       " 161.63854374208725,\n",
       " inf,\n",
       " 166.34346989679364,\n",
       " inf,\n",
       " 171.03903828378785,\n",
       " inf,\n",
       " 175.7252573231105,\n",
       " inf,\n",
       " 180.40213813144035,\n",
       " inf,\n",
       " 185.06969369480032,\n",
       " inf,\n",
       " 189.72793831890945,\n",
       " inf,\n",
       " 194.37688725630787,\n",
       " inf,\n",
       " 199.01655644836865,\n",
       " inf,\n",
       " 203.64696234348725,\n",
       " inf,\n",
       " 208.2681217667494,\n",
       " inf,\n",
       " 212.8800518250293,\n",
       " inf,\n",
       " 217.48276983686844,\n",
       " inf,\n",
       " 222.07629327997867,\n",
       " inf,\n",
       " 226.6606397514428,\n",
       " inf,\n",
       " 231.23582693720735,\n",
       " inf,\n",
       " 235.80187258844023,\n",
       " inf,\n",
       " 240.35879450304182,\n",
       " inf,\n",
       " 244.9066105110465,\n",
       " inf,\n",
       " 249.44533846300138,\n",
       " inf,\n",
       " 253.97499622065263,\n",
       " inf,\n",
       " 258.495601649411,\n",
       " inf,\n",
       " 263.00717261223275,\n",
       " inf,\n",
       " 267.5097269646072,\n",
       " inf,\n",
       " 272.0032825504358,\n",
       " inf,\n",
       " 276.4878571986222,\n",
       " inf,\n",
       " 280.96346872023094,\n",
       " inf,\n",
       " 285.430134906128,\n",
       " inf,\n",
       " 289.8878735249901,\n",
       " inf,\n",
       " 294.3367023216347,\n",
       " inf,\n",
       " 298.7766390156096,\n",
       " inf,\n",
       " 303.2077012999968,\n",
       " inf,\n",
       " 307.629906840398,\n",
       " inf,\n",
       " 312.04327327406907,\n",
       " inf,\n",
       " 316.44781820918314,\n",
       " inf,\n",
       " 320.84355922419996,\n",
       " inf,\n",
       " 325.2305138673291,\n",
       " inf,\n",
       " 329.6086996560641,\n",
       " inf,\n",
       " 333.97813407679104,\n",
       " inf,\n",
       " 338.3388345844469,\n",
       " inf,\n",
       " 342.6908186022343,\n",
       " inf,\n",
       " 347.0341035213679,\n",
       " inf,\n",
       " 351.36870670086864,\n",
       " inf,\n",
       " 355.6946454673815,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " 349.72279766545125,\n",
       " inf,\n",
       " 345.95997375843865,\n",
       " inf,\n",
       " 342.2046717914657,\n",
       " inf,\n",
       " 338.45687672670283,\n",
       " inf,\n",
       " 334.71657355643066,\n",
       " inf,\n",
       " 330.9837473029863,\n",
       " inf,\n",
       " 327.25838301868515,\n",
       " inf,\n",
       " 323.54046578577635,\n",
       " inf,\n",
       " 319.829980716366,\n",
       " inf,\n",
       " 316.12691295236164,\n",
       " inf,\n",
       " 312.43124766541916,\n",
       " inf,\n",
       " 308.7429700568642,\n",
       " inf,\n",
       " 305.0620653576471,\n",
       " inf,\n",
       " 301.3885188282769,\n",
       " inf,\n",
       " 297.7223157587567,\n",
       " inf,\n",
       " 294.06344146852837,\n",
       " inf,\n",
       " 290.41188130640984,\n",
       " inf,\n",
       " 286.76762065054294,\n",
       " inf,\n",
       " 283.13064490831863,\n",
       " inf,\n",
       " 279.5009395163307,\n",
       " inf,\n",
       " 275.87848994030884,\n",
       " inf,\n",
       " 272.26328167506364,\n",
       " inf,\n",
       " 268.65530024442967,\n",
       " inf,\n",
       " 265.05453120119427,\n",
       " inf,\n",
       " 261.4609601270579,\n",
       " inf,\n",
       " 257.8745726325534,\n",
       " inf,\n",
       " 254.29535435701032,\n",
       " inf,\n",
       " 250.72329096848006,\n",
       " inf,\n",
       " 247.15836816368264,\n",
       " inf,\n",
       " 243.60057166795022,\n",
       " inf,\n",
       " 240.04988723516945,\n",
       " inf,\n",
       " 236.50630064772236,\n",
       " inf,\n",
       " 232.96979771642512,\n",
       " inf,\n",
       " 229.44036428047997,\n",
       " inf,\n",
       " 225.9179862074098,\n",
       " inf,\n",
       " 222.40264939300295,\n",
       " inf,\n",
       " 218.8943397612588,\n",
       " inf,\n",
       " 215.39304326432753,\n",
       " inf,\n",
       " 211.89874588245706,\n",
       " inf,\n",
       " 208.41143362392728,\n",
       " inf,\n",
       " 204.9310925250119,\n",
       " inf,\n",
       " 201.4577086498997,\n",
       " inf,\n",
       " 197.99126809065606,\n",
       " inf,\n",
       " 194.53175696715508,\n",
       " inf,\n",
       " 191.0791614270328,\n",
       " inf,\n",
       " 187.63346764562377,\n",
       " inf,\n",
       " 184.1946618259127,\n",
       " inf,\n",
       " 180.7627301984709,\n",
       " inf,\n",
       " 177.33765902140567,\n",
       " inf,\n",
       " 173.919434580306,\n",
       " inf,\n",
       " 170.5080431881811,\n",
       " inf,\n",
       " 167.1034711854164,\n",
       " inf,\n",
       " 163.7057049397038,\n",
       " inf,\n",
       " 160.31473084599972,\n",
       " inf,\n",
       " 156.93053532646647,\n",
       " inf,\n",
       " 153.55310483041032,\n",
       " inf,\n",
       " 150.1824258342377,\n",
       " inf,\n",
       " 146.81848484139738,\n",
       " inf,\n",
       " 143.4612683823222,\n",
       " inf,\n",
       " 140.1107630143776,\n",
       " inf,\n",
       " 136.76695532180943,\n",
       " inf,\n",
       " 133.42983191568732,\n",
       " inf,\n",
       " 130.09937943385225,\n",
       " inf,\n",
       " 126.77558454086204,\n",
       " inf,\n",
       " 123.45843392793951,\n",
       " inf,\n",
       " 120.1479143129153,\n",
       " inf,\n",
       " 116.8440124401786,\n",
       " inf,\n",
       " 113.54671508062431,\n",
       " inf,\n",
       " 110.25600903159489,\n",
       " inf,\n",
       " 106.97188111682738,\n",
       " inf,\n",
       " 103.69431818640734,\n",
       " inf,\n",
       " 100.42330711671411,\n",
       " inf,\n",
       " 97.15883481036218,\n",
       " inf,\n",
       " 93.90088819615187,\n",
       " inf,\n",
       " 90.64945422902113,\n",
       " inf,\n",
       " 87.40451988998716,\n",
       " inf,\n",
       " 84.16607218609434,\n",
       " inf,\n",
       " 80.93409815037128,\n",
       " inf,\n",
       " 77.70858484176368,\n",
       " inf,\n",
       " 74.48951934509499,\n",
       " inf,\n",
       " 71.27688877100817,\n",
       " inf,\n",
       " 68.07068025591717,\n",
       " inf,\n",
       " 64.87088096195399,\n",
       " inf,\n",
       " 61.677478076915676,\n",
       " inf,\n",
       " 58.49045881421533,\n",
       " inf,\n",
       " 55.30981041283003,\n",
       " inf,\n",
       " 52.135520137251326,\n",
       " inf,\n",
       " 48.96757527742778,\n",
       " inf,\n",
       " 45.80596314872227,\n",
       " inf,\n",
       " 42.65067109185603,\n",
       " inf,\n",
       " 39.50168647285872,\n",
       " inf,\n",
       " 36.35899668301972,\n",
       " inf,\n",
       " 33.22258913883534,\n",
       " inf,\n",
       " 30.09245128195836,\n",
       " inf,\n",
       " 26.968570579150835,\n",
       " inf,\n",
       " 23.85093452222919,\n",
       " inf,\n",
       " 20.739530628014485,\n",
       " inf,\n",
       " 17.634346438288514,\n",
       " inf,\n",
       " 14.53536951973935,\n",
       " inf,\n",
       " 11.442587466132018,\n",
       " inf,\n",
       " 8.355989022516685,\n",
       " inf,\n",
       " 5.276126461350554,\n",
       " inf,\n",
       " 2.4603572365556876,\n",
       " inf,\n",
       " 66.47381627063304,\n",
       " inf,\n",
       " 63.277209246101954,\n",
       " inf,\n",
       " 60.086992246764936,\n",
       " inf,\n",
       " 56.903152498843575,\n",
       " inf,\n",
       " 53.725677254101775,\n",
       " inf,\n",
       " 50.554553789786304,\n",
       " inf,\n",
       " 47.3897694085791,\n",
       " inf,\n",
       " 44.23131143854754,\n",
       " inf,\n",
       " 41.0791672330902,\n",
       " inf,\n",
       " 37.93332417089056,\n",
       " inf,\n",
       " 34.79376965586062,\n",
       " inf,\n",
       " 31.66049111709624,\n",
       " inf,\n",
       " 28.533476008824046,\n",
       " inf,\n",
       " 25.412711810351112,\n",
       " inf,\n",
       " 22.298186026013518,\n",
       " inf,\n",
       " 19.18988618513053,\n",
       " inf,\n",
       " 16.08779984195239,\n",
       " inf,\n",
       " 12.991914575723674,\n",
       " inf,\n",
       " 9.90221804680326,\n",
       " inf,\n",
       " 6.818726276224391,\n",
       " inf,\n",
       " 3.755471915854934,\n",
       " inf,\n",
       " 6.080118337634629,\n",
       " inf,\n",
       " 3.0662426731745205,\n",
       " inf,\n",
       " 22.0377410921759,\n",
       " inf,\n",
       " 18.929961916083073,\n",
       " inf,\n",
       " 15.828395196331599,\n",
       " inf,\n",
       " 12.733028514334801,\n",
       " inf,\n",
       " 9.643849575976182,\n",
       " inf,\n",
       " 6.560896208749817,\n",
       " inf,\n",
       " 3.508866351553894,\n",
       " inf,\n",
       " 9.774852321818242,\n",
       " inf,\n",
       " 6.691626058169007,\n",
       " inf,\n",
       " 3.6339495238716677,\n",
       " inf,\n",
       " 7.934198053993868,\n",
       " inf,\n",
       " 4.856234340429596,\n",
       " inf,\n",
       " 2.4618683606017493,\n",
       " inf,\n",
       " 82.00380260128472,\n",
       " inf,\n",
       " 78.77615127064072,\n",
       " inf,\n",
       " 75.55495202052306,\n",
       " inf,\n",
       " 72.34019195315226,\n",
       " inf,\n",
       " 69.13185819653242,\n",
       " inf,\n",
       " 65.92993790439857,\n",
       " inf,\n",
       " 62.73441825616433,\n",
       " inf,\n",
       " 59.54528645687958,\n",
       " inf,\n",
       " 56.3625297371651,\n",
       " inf,\n",
       " 53.18613535317475,\n",
       " inf,\n",
       " 50.01609058653393,\n",
       " inf,\n",
       " 46.85238274429549,\n",
       " inf,\n",
       " 43.69499915888718,\n",
       " inf,\n",
       " 40.54392718805619,\n",
       " inf,\n",
       " 37.399154214826616,\n",
       " inf,\n",
       " 34.26066764744304,\n",
       " inf,\n",
       " 31.128454919323943,\n",
       " inf,\n",
       " 28.00250348900564,\n",
       " inf,\n",
       " 24.88280084009543,\n",
       " inf,\n",
       " 21.769334481227386,\n",
       " inf,\n",
       " 18.662091946003038,\n",
       " inf,\n",
       " 15.5610607929458,\n",
       " inf,\n",
       " 12.466228605849896,\n",
       " inf,\n",
       " 9.3775831955822,\n",
       " inf,\n",
       " 6.295213940932334,\n",
       " inf,\n",
       " 3.268697015989753,\n",
       " inf,\n",
       " 18.23300351802406,\n",
       " inf,\n",
       " 15.13283012685051,\n",
       " inf,\n",
       " 12.038853987343684,\n",
       " inf,\n",
       " 8.951063204984752,\n",
       " inf,\n",
       " 5.869692893664489,\n",
       " inf,\n",
       " 2.9124579205121286,\n",
       " inf,\n",
       " 38.360962164499846,\n",
       " inf,\n",
       " 35.2205529846869,\n",
       " inf,\n",
       " 32.086421486840244,\n",
       " inf,\n",
       " 28.958555121825082,\n",
       " inf,\n",
       " 25.836941365595987,\n",
       " inf,\n",
       " 22.721567719144193,\n",
       " inf,\n",
       " 19.61242170844376,\n",
       " inf,\n",
       " 16.509490884405146,\n",
       " inf,\n",
       " 13.412762822875514,\n",
       " inf,\n",
       " 10.32222515760583,\n",
       " inf,\n",
       " 7.237882182975685,\n",
       " inf,\n",
       " 4.167992924025997,\n",
       " inf,\n",
       " 4.304678665731185,\n",
       " inf,\n",
       " 3.6840960573583548,\n",
       " inf,\n",
       " 9.216676315466318,\n",
       " inf,\n",
       " 6.134685209428202,\n",
       " inf,\n",
       " 3.1352502095085812,\n",
       " inf,\n",
       " 26.676696548708833,\n",
       " inf,\n",
       " 23.55964425521186,\n",
       " inf,\n",
       " 20.44882295275492,\n",
       " inf,\n",
       " 17.34422018554843,\n",
       " inf,\n",
       " 14.245823522706178,\n",
       " inf,\n",
       " 11.153620564805905,\n",
       " inf,\n",
       " 8.067602267386937,\n",
       " inf,\n",
       " 4.9894212624093415,\n",
       " inf,\n",
       " 2.613582543177742,\n",
       " inf,\n",
       " 81.45611492000064,\n",
       " inf,\n",
       " 78.22955855174752,\n",
       " inf,\n",
       " 75.00945207325579,\n",
       " inf,\n",
       " 71.79578259115748,\n",
       " inf,\n",
       " 68.58853723785765,\n",
       " inf,\n",
       " 65.38770317148561,\n",
       " inf,\n",
       " 62.193267575841965,\n",
       " inf,\n",
       " 59.00521766034552,\n",
       " inf,\n",
       " 55.82354065998439,\n",
       " inf,\n",
       " 52.648223835268965,\n",
       " inf,\n",
       " 49.47925447217076,\n",
       " inf,\n",
       " 46.31661988207972,\n",
       " inf,\n",
       " 43.160307401750245,\n",
       " inf,\n",
       " 40.01030439325097,\n",
       " inf,\n",
       " 36.866598243912485,\n",
       " inf,\n",
       " 33.72917636628083,\n",
       " inf,\n",
       " 30.598026198065654,\n",
       " inf,\n",
       " 27.473135202086002,\n",
       " inf,\n",
       " 24.354490866226136,\n",
       " inf,\n",
       " 21.242080703379905,\n",
       " inf,\n",
       " 18.135892251404833,\n",
       " inf,\n",
       " 15.035913073071368,\n",
       " inf,\n",
       " 11.942130757539669,\n",
       " inf,\n",
       " 8.854533689740261,\n",
       " inf,\n",
       " 5.773496266774546,\n",
       " inf,\n",
       " 2.879323682603204,\n",
       " inf,\n",
       " 53.18842882953606,\n",
       " inf,\n",
       " 50.01837961893729,\n",
       " inf,\n",
       " 46.854667339660594,\n",
       " inf,\n",
       " 43.69727932415501,\n",
       " inf,\n",
       " 40.54620293018468,\n",
       " inf,\n",
       " 37.401425540792005,\n",
       " inf,\n",
       " 34.2629345642373,\n",
       " inf,\n",
       " 31.13071743395129,\n",
       " inf,\n",
       " 28.004761608489545,\n",
       " inf,\n",
       " 24.885054571473546,\n",
       " inf,\n",
       " 21.771583831547602,\n",
       " inf,\n",
       " 18.664336922328175,\n",
       " inf,\n",
       " 15.56330140234959,\n",
       " inf,\n",
       " 12.468464855590252,\n",
       " inf,\n",
       " 9.379815179701723,\n",
       " inf,\n",
       " 6.297485208418806,\n",
       " inf,\n",
       " 3.2916890311674223,\n",
       " inf,\n",
       " 24.963558960448267,\n",
       " inf,\n",
       " 21.84993129853024,\n",
       " inf,\n",
       " 18.742527780899138,\n",
       " inf,\n",
       " 15.641335965462474,\n",
       " inf,\n",
       " 12.546343435512716,\n",
       " inf,\n",
       " 9.457538055024123,\n",
       " inf,\n",
       " 6.3750361051733515,\n",
       " inf,\n",
       " 3.361224921913614,\n",
       " inf,\n",
       " 22.542215196085074,\n",
       " inf,\n",
       " 19.433427809276385,\n",
       " inf,\n",
       " 16.330854890934766,\n",
       " inf,\n",
       " 13.234484018448178,\n",
       " inf,\n",
       " 10.144302860101408,\n",
       " inf,\n",
       " 7.060332356425757,\n",
       " inf,\n",
       " 3.9990203795285097,\n",
       " inf,\n",
       " 7.227030829157408,\n",
       " inf,\n",
       " 4.160797801894867,\n",
       " inf,\n",
       " 5.672910526538752,\n",
       " inf,\n",
       " 2.852660662916554,\n",
       " inf,\n",
       " 66.31752217256408,\n",
       " inf,\n",
       " 63.12122793454418,\n",
       " inf,\n",
       " 59.931323091233835,\n",
       " inf,\n",
       " 56.74779487021234,\n",
       " inf,\n",
       " 53.57063052458111,\n",
       " inf,\n",
       " 50.39981733293531,\n",
       " inf,\n",
       " 47.235342599296864,\n",
       " inf,\n",
       " 44.07719365306249,\n",
       " inf,\n",
       " 40.925357848961504,\n",
       " inf,\n",
       " 37.77982256699954,\n",
       " inf,\n",
       " 34.64057521241574,\n",
       " inf,\n",
       " 31.507603215616694,\n",
       " inf,\n",
       " 28.380894032144067,\n",
       " inf,\n",
       " 25.260435142609516,\n",
       " inf,\n",
       " 22.146214052657403,\n",
       " inf,\n",
       " 19.038218292903515,\n",
       " inf,\n",
       " 15.9364354188978,\n",
       " inf,\n",
       " 12.840853011373211,\n",
       " inf,\n",
       " 9.75145883799832,\n",
       " inf,\n",
       " 6.668322108215019,\n",
       " inf,\n",
       " 3.631339942799076,\n",
       " inf,\n",
       " 15.291870944501474,\n",
       " inf,\n",
       " 12.197577026034702,\n",
       " inf,\n",
       " 9.109469209156973,\n",
       " inf,\n",
       " 6.027838046927636,\n",
       " inf,\n",
       " 3.0955797147770743,\n",
       " inf,\n",
       " 44.95367202750529,\n",
       " inf,\n",
       " 41.80008415667812,\n",
       " inf,\n",
       " 38.65280031020253,\n",
       " inf,\n",
       " 35.51180788631119,\n",
       " inf,\n",
       " 32.37709430843259,\n",
       " inf,\n",
       " 29.248647025134062,\n",
       " inf,\n",
       " 26.126453510072633,\n",
       " inf,\n",
       " 23.01050126194872,\n",
       " inf,\n",
       " 19.900777804452094,\n",
       " inf,\n",
       " 16.797270686208282,\n",
       " inf,\n",
       " 13.699967480787514,\n",
       " inf,\n",
       " 10.608855817727036,\n",
       " inf,\n",
       " 7.523939014923278,\n",
       " inf,\n",
       " 4.452994943919832,\n",
       " inf,\n",
       " 4.405421071356527,\n",
       " inf,\n",
       " 4.674681783858015,\n",
       " inf,\n",
       " 3.560782379253239,\n",
       " inf,\n",
       " 19.071754725672605,\n",
       " inf,\n",
       " 15.96990483024006,\n",
       " inf,\n",
       " 12.874255535051098,\n",
       " inf,\n",
       " 9.784794614980223,\n",
       " inf,\n",
       " 6.701595017797606,\n",
       " inf,\n",
       " 3.666353289445472,\n",
       " inf,\n",
       " 15.948600688563312,\n",
       " inf,\n",
       " 12.852993983122596,\n",
       " inf,\n",
       " 9.763575578539289,\n",
       " inf,\n",
       " 6.680423880949016,\n",
       " inf,\n",
       " 3.6478583641597297,\n",
       " inf,\n",
       " 16.83015452520215,\n",
       " inf,\n",
       " 13.732785595559726,\n",
       " inf,\n",
       " 10.641608339761277,\n",
       " inf,\n",
       " 7.556626199125118,\n",
       " inf,\n",
       " 4.485678131535202,\n",
       " inf,\n",
       " 4.461367398358651,\n",
       " inf,\n",
       " 4.601899515201897,\n",
       " inf,\n",
       " 3.9624472498910333,\n",
       " inf,\n",
       " 9.751107476193955,\n",
       " inf,\n",
       " 6.667987015991773,\n",
       " inf,\n",
       " 3.6384722070247255,\n",
       " inf,\n",
       " 17.84548919678138,\n",
       " inf,\n",
       " 14.746090618032293,\n",
       " inf,\n",
       " 11.652887743151291,\n",
       " inf,\n",
       " 8.565870342731944,\n",
       " inf,\n",
       " 5.486102928204948,\n",
       " inf,\n",
       " 2.8803932326650354,\n",
       " inf,\n",
       " 84.11220202208429,\n",
       " inf,\n",
       " 80.88033625705526,\n",
       " inf,\n",
       " 77.65493099369337,\n",
       " inf,\n",
       " 74.4359733174407,\n",
       " inf,\n",
       " 71.22345033955085,\n",
       " inf,\n",
       " 68.01734919705069,\n",
       " inf,\n",
       " 64.81765705267289,\n",
       " inf,\n",
       " 61.624361094815015,\n",
       " inf,\n",
       " 58.43744853748191,\n",
       " inf,\n",
       " 55.256906620244784,\n",
       " inf,\n",
       " 52.082722608175374,\n",
       " inf,\n",
       " 48.91488379180967,\n",
       " inf,\n",
       " 45.7533774870819,\n",
       " inf,\n",
       " 42.59819103529166,\n",
       " inf,\n",
       " 39.44931180303531,\n",
       " inf,\n",
       " 36.306727182168295,\n",
       " inf,\n",
       " 33.17042458974671,\n",
       " inf,\n",
       " 30.040391467983074,\n",
       " inf,\n",
       " 26.91661528418817,\n",
       " inf,\n",
       " 23.79908353073027,\n",
       " inf,\n",
       " 20.68778372497892,\n",
       " inf,\n",
       " 17.582703409259413,\n",
       " inf,\n",
       " 14.483830150803461,\n",
       " inf,\n",
       " 11.391151549672617,\n",
       " inf,\n",
       " 8.304659248099952,\n",
       " inf,\n",
       " 5.226348465566093,\n",
       " inf,\n",
       " 2.980409065233231,\n",
       " inf,\n",
       " 75.52167937134101,\n",
       " inf,\n",
       " 72.30698607692733,\n",
       " inf,\n",
       " 69.09871895624136,\n",
       " inf,\n",
       " 65.89686516334113,\n",
       " inf,\n",
       " 62.701411877973435,\n",
       " inf,\n",
       " 59.51234630551259,\n",
       " inf,\n",
       " 56.329655676901076,\n",
       " inf,\n",
       " 53.15332724861593,\n",
       " inf,\n",
       " 49.98334830260579,\n",
       " inf,\n",
       " 46.819706146240264,\n",
       " inf,\n",
       " 43.66238811226299,\n",
       " inf,\n",
       " 40.51138155874294,\n",
       " inf,\n",
       " 37.36667386901858,\n",
       " inf,\n",
       " 34.22825245164331,\n",
       " inf,\n",
       " 31.09610474034684,\n",
       " inf,\n",
       " 27.970218193980326,\n",
       " inf,\n",
       " 24.850580296459384,\n",
       " inf,\n",
       " 21.73717855672383,\n",
       " inf,\n",
       " 18.630000508683906,\n",
       " inf,\n",
       " 15.529033711163287,\n",
       " inf,\n",
       " 12.434265748932065,\n",
       " inf,\n",
       " 9.345684766735062,\n",
       " inf,\n",
       " 6.263547083013665,\n",
       " inf,\n",
       " 3.315244417962308,\n",
       " inf,\n",
       " 41.29706195441109,\n",
       " inf,\n",
       " 38.150783693499065,\n",
       " inf,\n",
       " 35.01079484446168,\n",
       " inf,\n",
       " 31.877082834750475]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Try different learning rates and compare the results. How does the learning rate influence the convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Try different regularization parameter values and compare the model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Compare zero initialization and random initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you need to implement weighted K-Neighbors Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that training a KNN classifier is simply memorizing a training sample. \n",
    "\n",
    "The process of applying a classifier for one object is to find the distances from it to all objects in the training data, then select the k nearest objects (neighbors) and return the most common class among these objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also give the nearest neighbors weights in accordance with the distance of the object to them. In the simplest case (as in your assignment), you can set the weights inversely proportional to that distance. \n",
    "\n",
    "$$w_{i} = \\frac{1}{d_{i} + eps},$$\n",
    "\n",
    "where $d_{i}$ is the distance between object and i-th nearest neighbor and $eps$ is the small value to prevent division by zero.\n",
    "\n",
    "In case of 'uniform' weights, all k nearest neighbors are equivalent (have equal weight, for example $w_{i} = 1, \\forall i \\in(1,k)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the probability of classes, it is necessary to normalize the weights of each class, dividing them by the sum:\n",
    "\n",
    "$$p_{i} = \\frac{w_{i}}{\\sum_{j=1}^{c}w_{j}},$$\n",
    "\n",
    "where $p_i$ is probability of i-th class and $c$ is the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2 points)** Implement the algorithm and use it to classify the digits. By implementing this algorithm, you will be able to classify numbers not only into \"even\" or \"odd\", but into their real representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomKNeighborsClassifier:\n",
    "    _estimator_type = \"classifier\"\n",
    "    \n",
    "    def __init__(self, n_neighbors=5, weights='uniform', eps=1e-9):\n",
    "        \"\"\"K-Nearest Neighbors classifier.\n",
    "        \n",
    "        Args:\n",
    "            n_neighbors: int, default=5\n",
    "                Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
    "            weights : {'uniform', 'distance'} or callable, default='uniform'\n",
    "                Weight function used in prediction.  Possible values:\n",
    "                - 'uniform' : uniform weights.  All points in each neighborhood\n",
    "                  are weighted equally.\n",
    "                - 'distance' : weight points by the inverse of their distance.\n",
    "                  in this case, closer neighbors of a query point will have a\n",
    "                  greater influence than neighbors which are further away.\n",
    "            eps : float, default=1e-5\n",
    "                Epsilon to prevent division by 0 \n",
    "        \"\"\"\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "        self.eps = eps\n",
    "        \n",
    "    \n",
    "    def get_pairwise_distances(self, X, Y):\n",
    "        \"\"\"\n",
    "        Returnes matrix of the pairwise distances between the rows from both X and Y.\n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "            Y: numpy array of shape (k_samples, n_features)\n",
    "        Returns:\n",
    "            P: numpy array of shape (n_samples, k_samples)\n",
    "                Matrix in which (i, j) value is the distance \n",
    "                between i'th row from the X and j'th row from the Y.\n",
    "        \"\"\"\n",
    "        # <your code>\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def get_class_weights(self, y, weights):\n",
    "        \"\"\"\n",
    "        Returns a vector with sum of weights for each class \n",
    "        Args:\n",
    "            y: numpy array of shape (n_samles,)\n",
    "            weights: numpy array of shape (n_samples,)\n",
    "                The weights of the corresponding points of y.\n",
    "        Returns:\n",
    "            p: numpy array of shape (n_classes)\n",
    "                Array where the value at the i-th position \n",
    "                corresponds to the weight of the i-th class.\n",
    "        \"\"\"\n",
    "        # <your code>\n",
    "        pass\n",
    "            \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Target vector.        \n",
    "        \"\"\"\n",
    "        self.points = X\n",
    "        self.y = y\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict positive class probabilities.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples, n_classes)\n",
    "                Vector containing positive class probabilities.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'points'):\n",
    "            P = self.get_pairwise_distances(X, self.points)\n",
    "            \n",
    "            weights_of_points = np.ones(P.shape)\n",
    "            if self.weights == 'distance':\n",
    "                weights_of_points = 'your code'\n",
    "                \n",
    "            # <your code>\n",
    "            pass\n",
    "        \n",
    "        else: \n",
    "            raise NotFittedError(\"CustomKNeighborsClassifier instance is not fitted yet\")\n",
    "            \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing predicted class labels.\n",
    "        \"\"\"\n",
    "        # <your code>\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomKNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(model.get_pairwise_distances(np.array([[0  , 1]  , [1, 1]]), \n",
    "                                                np.array([[0.5, 0.5], [1, 0]])),\n",
    "                   np.array([[0.70710678, 1.41421356],\n",
    "                             [0.70710678, 1.        ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_ = ['one', 'two', 'three']\n",
    "assert np.allclose(model.get_class_weights(np.array(['one', 'one', 'three', 'two']), np.array([1, 1, 0, 4])), \n",
    "                   np.array([2,4,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "_, axes = plt.subplots(nrows=3, ncols=7, figsize=(10, 5))\n",
    "for ax, image, label in zip(axes.flatten(), X, y):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image.reshape((8, 8)), cmap=plt.cm.gray_r if label % 2 else plt.cm.afmhot_r)\n",
    "    ax.set_title(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "knn.fit(X_train, list(map(str, y_train)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(model.predict_proba(X_test), knn.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, test_acc = fit_evaluate(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_acc == 1\n",
    "assert test_acc > 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Take a look at the confusion matrix and tell what numbers the model confuses and why this happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Try different n_neighbors parameters and compare the output probabilities of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Compare both 'uniform' and 'distance' weights and share your thoughts in what situations which parameter can be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Suggest another distance measurement function that could improve the quality of the classification for this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Suggest different task and distance function that you think would be suitable for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Synthetic Titanic Survival Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Read the description here: https://www.kaggle.com/c/tabular-playground-series-apr-2021/data. Download the dataset and place it in the *data/titanic/* folder in your working directory.\n",
    "You will use train.csv for model training and validation. The test set is used for model testing: once the model is trained, you can predict whether a passenger survived or not for each passenger in the test set, and submit the predictions: https://www.kaggle.com/c/tabular-playground-series-apr-2021/overview/evaluation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(PATH, 'titanic', 'train.csv')).set_index('PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** How many females and males are there in the dataset? What about the survived passengers? Is there any relationship between the gender and the survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Plot age distribution of the passengers. What is the average and the median age of survived and deceased passengers? Do age distributions differ for survived and deceased passengers? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 point)** Explore \"passenger class\" and \"embarked\" features. What class was \"the safest\"? Is there any relationship between the embarkation port and the survival? Provide the corresponding visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Find the percentage of missing values for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the ways to handle these missing values for modelling and write your answer below. Which methods would you suggest? What are their advantages and disadvantages?\n",
    "\n",
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.5 points)** Prepare the features and train two models (KNN and Logistic Regression) to predict the survival. Compare the results. Use accuracy as a metric. Don't forget about cross-validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 + X points)** Try more feature engineering and hyperparameter tuning to improve the results. You may use either KNN or Logistic Regression (or both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the best model, load the test set and make the predictions. Submit them to kaggle and see the results :)\n",
    "\n",
    "**Note**. X points will depend on your kaggle public leaderboard score.\n",
    "$$ f(score) = 1.0, \\ \\ 0.79 \\leq score < 0.80,$$\n",
    "$$ f(score) = 2.5, \\ \\ 0.80 \\leq score < 0.81,$$ \n",
    "$$ f(score) = 4.0, \\ \\ 0.81 \\leq score $$ \n",
    "Your code should generate the output submitted to kaggle. Fix random seeds to make the results reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
