{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification. Linear models and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementing Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you need to implement Logistic Regression with l2 regularization using gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression loss:\n",
    "$$ L(w) = \\dfrac{1}{N}\\sum_{i=1}^N \\log(1 + e^{-\\langle w, x_i \\rangle y_i}) + \\frac{1}{2C} \\lVert w \\rVert^2  \\to \\min_w$$\n",
    "$$\\langle w, x_i \\rangle = \\sum_{j=1}^n w_{j}x_{ij} + w_{0},$$ $$ y_{i} \\in \\{-1, 1\\}$$ where $n$ is the number of features and $N$ is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent step:\n",
    "$$w^{(t+1)} := w^{(t)} + \\dfrac{\\eta}{N}\\sum_{i=1}^N y_ix_i \\Big(1 - \\dfrac{1}{1 + exp(-\\langle w^{(t)}, x_i \\rangle y_i)}\\Big) - \\eta \\frac{1}{C} w,$$\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2 points)** Implement the algorithm and use it to classify the digits (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) into \"even\" and \"odd\" categories. \"Even\" and \"Odd\" classes  should correspond to {-1, 1} labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopping criteria: either the number of iterations exceeds *max_iter* or $||w^{(t+1)} - w^{(t)}||_2 < tol$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import NotFittedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### NEW ONE \n",
    "class CustomLogisticRegression:\n",
    "    _estimator_type = \"classifier\"\n",
    "    \n",
    "    def __init__(self, eta=0.001, max_iter=1000, C=1.0, tol=1e-5, random_state=42, zero_init=False):\n",
    "        \"\"\"Logistic Regression classifier.\n",
    "        \n",
    "        Args:\n",
    "            eta: float, default=0.001\n",
    "                Learning rate.\n",
    "            max_iter: int, default=1000\n",
    "                Maximum number of iterations taken for the solvers to converge.\n",
    "            C: float, default=1.0\n",
    "                Inverse of regularization strength; must be a positive float.\n",
    "                Smaller values specify stronger regularization.\n",
    "            tol: float, default=1e-5\n",
    "                Tolerance for stopping criteria.\n",
    "            random_state: int, default=42\n",
    "                Random state.\n",
    "            zero_init: bool, default=False\n",
    "                Zero weight initialization.\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "        self.random_state = np.random.RandomState(seed=random_state)\n",
    "        self.zero_init = zero_init\n",
    "         \n",
    "    def get_sigmoid(self, X, weights):\n",
    "        return 1 / (1 + np.exp(-X.dot(weights.T)))\n",
    "\n",
    "\n",
    "    def get_loss(self, X, weights, y):\n",
    "        \"\"\"Calculate the loss.\"\"\"\n",
    "        mult_weights = sum(X.dot(weights.T))\n",
    "        return np.average(np.log(1+np.exp(-mult_weights*y))) + 0.5 / self.C * np.linalg.norm(weights)\n",
    "                    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Target vector.        \n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "        X_ext = np.hstack([np.ones((X.shape[0], 1)), X]) # a constant feature is included to handle intercept\n",
    "        num_features = X_ext.shape[1]\n",
    "        if self.zero_init:\n",
    "            self.weights_ = np.zeros(num_features) \n",
    "        else:\n",
    "            weight_threshold = 1.0 / (2 * num_features)\n",
    "            self.weights_ = self.random_state.uniform(low=-weight_threshold,\n",
    "                                                      high=weight_threshold, size=num_features)\n",
    "\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            loss_history.append(self.get_loss(X_ext, self.weights_, y))\n",
    "            \n",
    "            arr = (1-1/(1+np.exp(-sum(X_ext.dot(self.weights_.T))*y)))\n",
    "            arr_2 = np.average(y[:,None]*X_ext*arr[:,None],axis=0)\n",
    "            delta = -arr_2 + 1/self.C * self.weights_\n",
    "            self.weights_ -= self.eta * delta\n",
    "                    \n",
    "#                     # код тянучки\n",
    "#             delta = y[i] * X_ext[i] * (1 -1 / (1 + np.exp(-y[i] * (np.dot(Xext[i], self.weights[i]))))) \n",
    "#             self.weights_ -= self.eta * delta \n",
    "                    \n",
    "            if (i > self.max_iter) or np.linalg.norm(delta) < self.tol:   \n",
    "                print('!!!!! tol')\n",
    "                break\n",
    "        return loss_history, self.weights_\n",
    "     \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict positive class probabilities.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing positive class probabilities.\n",
    "        \"\"\"\n",
    "        X_ext = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        if hasattr(self, 'weights_'):\n",
    "            return self.get_sigmoid(X_ext, self.weights_)\n",
    "        else: \n",
    "            raise NotFittedError(\"CustomLogisticRegression instance is not fitted yet\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing predicted class labels.\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return [1 if i>0.5 else -1 for i in proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 21 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = datasets.load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "_, axes = plt.subplots(nrows=3, ncols=7, figsize=(10, 5))\n",
    "for ax, image, label in zip(axes.flatten(), X, y):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image.reshape((8, 8)), cmap=plt.cm.gray_r if label % 2 else plt.cm.afmhot_r)\n",
    "    ax.set_title(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "y_train = (y_train % 2) * 2 - 1\n",
    "y_test = (y_test % 2) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.unique(y_train) == [-1, 1]).all()\n",
    "assert (np.unique(y_test) == [-1, 1]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# изменила\n",
    "def fit_evaluate(clf, X_train, y_train, X_test, y_test):\n",
    "    loss, weights = clf.fit(X_train, y_train)\n",
    "    disp = metrics.plot_confusion_matrix(clf, X_test, y_test, normalize='true')\n",
    "    disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics.accuracy_score(y_pred=clf.predict(X_train), y_true=y_train), \\\n",
    "           metrics.accuracy_score(y_pred=clf.predict(X_test), y_true=y_test), \\\n",
    "           loss, weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = CustomLogisticRegression(max_iter=1, zero_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(lr_clf.get_sigmoid(np.array([[0.5, 0, 1.0], [0.3, 1.3, 1.0]]), np.array([0.5, -0.5, 0.1])),\n",
    "                   np.array([0.58662, 0.40131]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_ext.shape 1437 65\n"
     ]
    }
   ],
   "source": [
    "lr_clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(lr_clf.weights_, np.array([ 3.1000e-06,  0.0000e+00,  4.1800e-05,  5.4770e-04,  2.2130e-04,\n",
    "        4.8750e-04,  1.3577e-03,  5.9780e-04,  5.6400e-05, -7.0000e-07,\n",
    "        1.6910e-04,  2.5190e-04, -4.3700e-04,  3.6190e-04,  1.0049e-03,\n",
    "        4.2280e-04,  2.5700e-05,  3.0000e-07, -1.1500e-05, -7.2440e-04,\n",
    "       -2.6200e-04,  8.7540e-04,  4.1540e-04, -8.4200e-05, -5.2000e-06,\n",
    "        0.0000e+00, -2.2160e-04, -5.7130e-04,  9.8570e-04,  1.3507e-03,\n",
    "        5.0210e-04, -1.7050e-04, -1.0000e-06,  0.0000e+00, -6.7810e-04,\n",
    "       -1.0515e-03, -4.4500e-05,  3.7160e-04,  4.2100e-04, -8.1800e-05,\n",
    "        0.0000e+00, -5.2000e-06, -5.3410e-04, -2.0393e-03, -8.4310e-04,\n",
    "        1.0400e-04, -1.2390e-04, -1.7880e-04, -1.3200e-05, -4.5000e-06,\n",
    "       -9.4300e-05, -1.1127e-03, -5.0900e-04, -2.1850e-04, -5.6050e-04,\n",
    "       -3.9560e-04, -1.7700e-05, -3.0000e-07,  2.6800e-05,  6.3920e-04,\n",
    "        1.8090e-04, -7.3660e-04, -5.3930e-04, -3.7060e-04, -2.8200e-05]), atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_ext.shape 1437 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wl/27ymcjln6px3tcfww0c_jg5c0000gn/T/ipykernel_16017/48250492.py:37: RuntimeWarning: overflow encountered in exp\n",
      "  return np.average(np.log(1+np.exp(-mult_weights*y))) + 0.5 / self.C * np.linalg.norm(weights)\n",
      "/var/folders/wl/27ymcjln6px3tcfww0c_jg5c0000gn/T/ipykernel_16017/48250492.py:62: RuntimeWarning: overflow encountered in exp\n",
      "  arr = (1-1/(1+np.exp(-sum(X_ext.dot(self.weights_.T))*y)))\n",
      "/Users/sonya/Library/Python/3.8/lib/python/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc, test_acc,loss, weights = fit_evaluate(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8308977035490606, 0.8611111111111112)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wl/27ymcjln6px3tcfww0c_jg5c0000gn/T/ipykernel_16017/4215115562.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert min(train_acc, test_acc) > 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Visualize the loss history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3420784690213354,\n",
       " inf,\n",
       " 17.042164255839094,\n",
       " inf,\n",
       " 13.941574928503574,\n",
       " inf,\n",
       " 10.847933567435982,\n",
       " inf,\n",
       " 7.761021347786352,\n",
       " inf,\n",
       " 4.681074241471489,\n",
       " inf,\n",
       " 1.799473865397442,\n",
       " inf,\n",
       " 54.400272940351556,\n",
       " inf,\n",
       " 51.227269273800694,\n",
       " inf,\n",
       " 48.060708852348775,\n",
       " inf,\n",
       " 44.900553978535605,\n",
       " inf,\n",
       " 41.74677429344517,\n",
       " inf,\n",
       " 38.59934438059853,\n",
       " inf,\n",
       " 35.458242218617144,\n",
       " inf,\n",
       " 32.323448166319665,\n",
       " inf,\n",
       " 29.19494428491459,\n",
       " inf,\n",
       " 26.072713876801078,\n",
       " inf,\n",
       " 22.956741165970737,\n",
       " inf,\n",
       " 19.847011072632966,\n",
       " inf,\n",
       " 16.74350905165455,\n",
       " inf,\n",
       " 13.646220974951738,\n",
       " inf,\n",
       " 10.555133048033019,\n",
       " inf,\n",
       " 7.470233450247888,\n",
       " inf,\n",
       " 4.392364515713575,\n",
       " inf,\n",
       " 1.6996822260717839,\n",
       " inf,\n",
       " 78.67538214363769,\n",
       " inf,\n",
       " 75.45432985559322,\n",
       " inf,\n",
       " 72.23972053300177,\n",
       " inf,\n",
       " 69.03154086754607,\n",
       " inf,\n",
       " 65.82977763501682,\n",
       " inf,\n",
       " 62.634417685876564,\n",
       " inf,\n",
       " 59.4454479375732,\n",
       " inf,\n",
       " 56.2628553682298,\n",
       " inf,\n",
       " 53.08662701143263,\n",
       " inf,\n",
       " 49.916749951903256,\n",
       " inf,\n",
       " 46.75321132187881,\n",
       " inf,\n",
       " 43.595998298069986,\n",
       " inf,\n",
       " 40.4450980990908,\n",
       " inf,\n",
       " 37.300497983277175,\n",
       " inf,\n",
       " 34.162185246822645,\n",
       " inf,\n",
       " 31.030147222181352,\n",
       " inf,\n",
       " 27.90437127669332,\n",
       " inf,\n",
       " 24.784844811391626,\n",
       " inf,\n",
       " 21.67155525996922,\n",
       " inf,\n",
       " 18.5644900878748,\n",
       " inf,\n",
       " 15.4636367915222,\n",
       " inf,\n",
       " 12.368982897702724,\n",
       " inf,\n",
       " 9.280516017586015,\n",
       " inf,\n",
       " 6.198251344099872,\n",
       " inf,\n",
       " 3.135835874031103,\n",
       " inf,\n",
       " 5.316775351968156,\n",
       " inf,\n",
       " 2.3227024605270907,\n",
       " inf,\n",
       " 27.005379651277078,\n",
       " inf,\n",
       " 23.887658081755525,\n",
       " inf,\n",
       " 20.77616932703076,\n",
       " inf,\n",
       " 17.67090090075522,\n",
       " inf,\n",
       " 14.571840343466476,\n",
       " inf,\n",
       " 11.478975223072139,\n",
       " inf,\n",
       " 8.392293492386594,\n",
       " inf,\n",
       " 5.311962879182982,\n",
       " inf,\n",
       " 2.324571713033376,\n",
       " inf,\n",
       " 29.003667633237555,\n",
       " inf,\n",
       " 25.881954993060457,\n",
       " inf,\n",
       " 22.76648296269334,\n",
       " inf,\n",
       " 19.65723905258871,\n",
       " inf,\n",
       " 16.554210799064865,\n",
       " inf,\n",
       " 13.457385764189981,\n",
       " inf,\n",
       " 10.366751542887949,\n",
       " inf,\n",
       " 7.2822993908328755,\n",
       " inf,\n",
       " 4.205834048119419,\n",
       " inf,\n",
       " 1.8896085287588846,\n",
       " inf,\n",
       " 78.20182857600425,\n",
       " inf,\n",
       " 74.98177113502135,\n",
       " inf,\n",
       " 71.7681507880877,\n",
       " inf,\n",
       " 68.56095465921157,\n",
       " inf,\n",
       " 65.36016989858625,\n",
       " inf,\n",
       " 62.1657836825035,\n",
       " inf,\n",
       " 58.97778321327544,\n",
       " inf,\n",
       " 55.79615571915789,\n",
       " inf,\n",
       " 52.6208884542782,\n",
       " inf,\n",
       " 49.45196869855652,\n",
       " inf,\n",
       " 46.28938375764122,\n",
       " inf,\n",
       " 43.13312096283551,\n",
       " inf,\n",
       " 39.983167671028255,\n",
       " inf,\n",
       " 36.83951126463138,\n",
       " inf,\n",
       " 33.70213915151019,\n",
       " inf,\n",
       " 30.5710387649218,\n",
       " inf,\n",
       " 27.446197563448607,\n",
       " inf,\n",
       " 24.327603030938185,\n",
       " inf,\n",
       " 21.215242676443392,\n",
       " inf,\n",
       " 18.109104034157305,\n",
       " inf,\n",
       " 15.00917466335764,\n",
       " inf,\n",
       " 11.915442148732833,\n",
       " inf,\n",
       " 8.827894295141533,\n",
       " inf,\n",
       " 5.746616946759848,\n",
       " inf,\n",
       " 2.7194940270631447,\n",
       " inf,\n",
       " 17.114732817175458,\n",
       " inf,\n",
       " 14.01679170479654,\n",
       " inf,\n",
       " 10.925043459851416,\n",
       " inf,\n",
       " 7.839477199984143,\n",
       " inf,\n",
       " 4.760830325672801,\n",
       " inf,\n",
       " 2.0239401931389978,\n",
       " inf,\n",
       " 75.41150898750381,\n",
       " inf,\n",
       " 72.19703294714829,\n",
       " inf,\n",
       " 68.9889827134601,\n",
       " inf,\n",
       " 65.78734543920993,\n",
       " inf,\n",
       " 62.592108302928864,\n",
       " inf,\n",
       " 59.40325850885322,\n",
       " inf,\n",
       " 56.220783286868134,\n",
       " inf,\n",
       " 53.04466989245428,\n",
       " inf,\n",
       " 49.87490560663282,\n",
       " inf,\n",
       " 46.71147773591362,\n",
       " inf,\n",
       " 43.55437361223671,\n",
       " inf,\n",
       " 40.4035805929245,\n",
       " inf,\n",
       " 37.25908606062493,\n",
       " inf,\n",
       " 34.120877423258996,\n",
       " inf,\n",
       " 30.988942113970285,\n",
       " inf,\n",
       " 27.86326759106764,\n",
       " inf,\n",
       " 24.743841337978925,\n",
       " inf,\n",
       " 21.630650863194052,\n",
       " inf,\n",
       " 18.523683700214022,\n",
       " inf,\n",
       " 15.42292740750286,\n",
       " inf,\n",
       " 12.328369568639447,\n",
       " inf,\n",
       " 9.239997897776975,\n",
       " inf,\n",
       " 6.157853277630396,\n",
       " inf,\n",
       " 3.108119908192014,\n",
       " inf,\n",
       " 9.913810576058642,\n",
       " inf,\n",
       " 6.830279719377134,\n",
       " inf,\n",
       " 3.759889279358529,\n",
       " inf,\n",
       " 3.3973903725844536,\n",
       " inf,\n",
       " 5.987172202066241,\n",
       " inf,\n",
       " 2.949978072361407,\n",
       " inf,\n",
       " 14.068052682953676,\n",
       " inf,\n",
       " 10.976203603870676,\n",
       " inf,\n",
       " 7.890536902376116,\n",
       " inf,\n",
       " 4.811907911945332,\n",
       " inf,\n",
       " 2.1233583570568113,\n",
       " inf,\n",
       " 79.43597211497838,\n",
       " inf,\n",
       " 76.21345261903161,\n",
       " inf,\n",
       " 72.99737496929309,\n",
       " inf,\n",
       " 69.78772628784121,\n",
       " inf,\n",
       " 66.58449372251962,\n",
       " inf,\n",
       " 63.38766444688174,\n",
       " inf,\n",
       " 60.19722566013832,\n",
       " inf,\n",
       " 57.013164587107205,\n",
       " inf,\n",
       " 53.835468478158724,\n",
       " inf,\n",
       " 50.66412460916962,\n",
       " inf,\n",
       " 47.49912028146283,\n",
       " inf,\n",
       " 44.34044282176684,\n",
       " inf,\n",
       " 41.1880795821526,\n",
       " inf,\n",
       " 38.0420179399922,\n",
       " inf,\n",
       " 34.90224529790405,\n",
       " inf,\n",
       " 31.76874908370045,\n",
       " inf,\n",
       " 28.64151675033927,\n",
       " inf,\n",
       " 25.52053577587235,\n",
       " inf,\n",
       " 22.405793663393172,\n",
       " inf,\n",
       " 19.297277940990313,\n",
       " inf,\n",
       " 16.194976161694246,\n",
       " inf,\n",
       " 13.098875903468471,\n",
       " inf,\n",
       " 10.008964797806229,\n",
       " inf,\n",
       " 6.925244938321362,\n",
       " inf,\n",
       " 3.8548870434328797,\n",
       " inf,\n",
       " 3.5765539383816356,\n",
       " inf,\n",
       " 5.429332839759841,\n",
       " inf,\n",
       " 2.4974201023050067,\n",
       " inf,\n",
       " 44.15096837161998,\n",
       " inf,\n",
       " 40.998984180686385,\n",
       " inf,\n",
       " 37.85330082339442,\n",
       " inf,\n",
       " 34.713905704053644,\n",
       " inf,\n",
       " 31.580786252154958,\n",
       " inf,\n",
       " 28.453929922328168,\n",
       " inf,\n",
       " 25.33332419428763,\n",
       " inf,\n",
       " 22.218956572783462,\n",
       " inf,\n",
       " 19.11081458755204,\n",
       " inf,\n",
       " 16.0088857932635,\n",
       " inf,\n",
       " 12.91315776956781,\n",
       " inf,\n",
       " 9.82361816737086,\n",
       " inf,\n",
       " 6.740278041697033,\n",
       " inf,\n",
       " 3.6747277012553896,\n",
       " inf,\n",
       " 5.078717148243588,\n",
       " inf,\n",
       " 2.3106582813627523,\n",
       " inf,\n",
       " 72.50074671724559,\n",
       " inf,\n",
       " 69.29209149076532,\n",
       " inf,\n",
       " 66.0898503798312,\n",
       " inf,\n",
       " 62.894010562388296,\n",
       " inf,\n",
       " 59.70455924201628,\n",
       " inf,\n",
       " 56.52148364787828,\n",
       " inf,\n",
       " 53.344771034674096,\n",
       " inf,\n",
       " 50.174408682587355,\n",
       " inf,\n",
       " 47.01038389722785,\n",
       " inf,\n",
       " 43.852684009594086,\n",
       " inf,\n",
       " 40.701296376007576,\n",
       " inf,\n",
       " 37.55620837807127,\n",
       " inf,\n",
       " 34.41740742261901,\n",
       " inf,\n",
       " 31.284880941658518,\n",
       " inf,\n",
       " 28.158616392328305,\n",
       " inf,\n",
       " 25.038601256842977,\n",
       " inf,\n",
       " 21.924823042442743,\n",
       " inf,\n",
       " 18.81726928134605,\n",
       " inf,\n",
       " 15.715927530699059,\n",
       " inf,\n",
       " 12.620785372708296,\n",
       " inf,\n",
       " 9.531830511981076,\n",
       " inf,\n",
       " 6.449099725743514,\n",
       " inf,\n",
       " 3.3967789071137138,\n",
       " inf,\n",
       " 9.476747602860556,\n",
       " inf,\n",
       " 6.394133730619708,\n",
       " inf,\n",
       " 3.3452234171423845,\n",
       " inf,\n",
       " 10.615714886935056,\n",
       " inf,\n",
       " 7.530773894269933,\n",
       " inf,\n",
       " 4.454872904431337,\n",
       " inf,\n",
       " 2.5346399323970923,\n",
       " inf,\n",
       " 54.93281234313975,\n",
       " inf,\n",
       " 51.759275686523914,\n",
       " inf,\n",
       " 48.592082938742514,\n",
       " inf,\n",
       " 45.43122141818275,\n",
       " inf,\n",
       " 42.27667846858681,\n",
       " inf,\n",
       " 39.128441459000264,\n",
       " inf,\n",
       " 35.986497783717894,\n",
       " inf,\n",
       " 32.8508348622378,\n",
       " inf,\n",
       " 29.721440139207676,\n",
       " inf,\n",
       " 26.598301084377457,\n",
       " inf,\n",
       " 23.481405192543264,\n",
       " inf,\n",
       " 20.3707399835072,\n",
       " inf,\n",
       " 17.26629300201671,\n",
       " inf,\n",
       " 14.168051817726763,\n",
       " inf,\n",
       " 11.076004030157284,\n",
       " inf,\n",
       " 7.990139790691597,\n",
       " inf,\n",
       " 4.911711084867607,\n",
       " inf,\n",
       " 2.3811183019276534,\n",
       " inf,\n",
       " 84.56944671416339,\n",
       " inf,\n",
       " 81.33666650615726,\n",
       " inf,\n",
       " 78.11034863341156,\n",
       " inf,\n",
       " 74.89048017760553,\n",
       " inf,\n",
       " 71.6770482462521,\n",
       " inf,\n",
       " 68.47003997263457,\n",
       " inf,\n",
       " 65.26944251575975,\n",
       " inf,\n",
       " 62.07524306030513,\n",
       " inf,\n",
       " 58.88742881656354,\n",
       " inf,\n",
       " 55.7059870204034,\n",
       " inf,\n",
       " 52.53090493320638,\n",
       " inf,\n",
       " 49.3621698418207,\n",
       " inf,\n",
       " 46.1997690585096,\n",
       " inf,\n",
       " 43.043689920902146,\n",
       " inf,\n",
       " 39.893919791939176,\n",
       " inf,\n",
       " 36.75044605982506,\n",
       " inf,\n",
       " 33.61325613797755,\n",
       " inf,\n",
       " 30.48233746497462,\n",
       " inf,\n",
       " 27.357677504507645,\n",
       " inf,\n",
       " 24.239263745330152,\n",
       " inf,\n",
       " 21.127083701202828,\n",
       " inf,\n",
       " 18.02112491085347,\n",
       " inf,\n",
       " 14.921374937918074,\n",
       " inf,\n",
       " 11.82782137217451,\n",
       " inf,\n",
       " 8.740452475514642,\n",
       " inf,\n",
       " 5.659580732709898,\n",
       " inf,\n",
       " 2.7379619035202767,\n",
       " inf,\n",
       " 46.945777575418596,\n",
       " inf,\n",
       " 43.788207223999514,\n",
       " inf,\n",
       " 40.63694886130714,\n",
       " inf,\n",
       " 37.49198986960066,\n",
       " inf,\n",
       " 34.353317656367665,\n",
       " inf,\n",
       " 31.22091965426841,\n",
       " inf,\n",
       " 28.09478332108919,\n",
       " inf,\n",
       " 24.974896139684578,\n",
       " inf,\n",
       " 21.86124561793402,\n",
       " inf,\n",
       " 18.753819288688305,\n",
       " inf,\n",
       " 15.652604709725962,\n",
       " inf,\n",
       " 12.557589464009478,\n",
       " inf,\n",
       " 9.468761320892519,\n",
       " inf,\n",
       " 6.386189220828718,\n",
       " inf,\n",
       " 3.3496396654595255,\n",
       " inf,\n",
       " 14.965459727695889,\n",
       " inf,\n",
       " 11.871818100825266,\n",
       " inf,\n",
       " 8.78436132983104,\n",
       " inf,\n",
       " 5.703407726727295,\n",
       " inf,\n",
       " 2.784324028099938,\n",
       " inf,\n",
       " 47.59928565648744,\n",
       " inf,\n",
       " 44.440409003653784,\n",
       " inf,\n",
       " 41.28784694995993,\n",
       " inf,\n",
       " 38.14158687246769,\n",
       " inf,\n",
       " 35.00161617346966,\n",
       " inf,\n",
       " 31.86792228044558,\n",
       " inf,\n",
       " 28.74049264600155,\n",
       " inf,\n",
       " 25.619314747836544,\n",
       " inf,\n",
       " 22.504376088674334,\n",
       " inf,\n",
       " 19.395664196225955,\n",
       " inf,\n",
       " 16.29316662313324,\n",
       " inf,\n",
       " 13.196870947018382,\n",
       " inf,\n",
       " 10.10676481984033,\n",
       " inf,\n",
       " 7.02286084045903,\n",
       " inf,\n",
       " 3.9575100707927247,\n",
       " inf,\n",
       " 5.648484668069149,\n",
       " inf,\n",
       " 2.761745169690566,\n",
       " inf,\n",
       " 54.54659208413258,\n",
       " inf,\n",
       " 51.37382780342385,\n",
       " inf,\n",
       " 48.20740588268336,\n",
       " inf,\n",
       " 45.04731364348767,\n",
       " inf,\n",
       " 41.89353843275594,\n",
       " inf,\n",
       " 38.74606762270384,\n",
       " inf,\n",
       " 35.604888610784926,\n",
       " inf,\n",
       " 32.46998881965334,\n",
       " inf,\n",
       " 29.34135569710006,\n",
       " inf,\n",
       " 26.21897671601285,\n",
       " inf,\n",
       " 23.102839374317554,\n",
       " inf,\n",
       " 19.992931194937405,\n",
       " inf,\n",
       " 16.88923972573243,\n",
       " inf,\n",
       " 13.79175253947645,\n",
       " inf,\n",
       " 10.700457250264368,\n",
       " inf,\n",
       " 7.615349790634228,\n",
       " inf,\n",
       " 4.540553221224095,\n",
       " inf,\n",
       " 3.0988644319605503,\n",
       " inf,\n",
       " 30.02669854404461,\n",
       " inf,\n",
       " 26.902949581268217,\n",
       " inf,\n",
       " 23.785444996229703,\n",
       " inf,\n",
       " 20.674172306380115,\n",
       " inf,\n",
       " 17.569119054122517,\n",
       " inf,\n",
       " 14.470272806765982,\n",
       " inf,\n",
       " 11.377621160892907,\n",
       " inf,\n",
       " 8.291153966366435,\n",
       " inf,\n",
       " 5.211974148236539,\n",
       " inf,\n",
       " 2.622068549301046,\n",
       " inf,\n",
       " 84.22280616325763,\n",
       " inf,\n",
       " 80.99071916728397,\n",
       " inf,\n",
       " 77.76509311681497,\n",
       " inf,\n",
       " 74.54591509638344,\n",
       " inf,\n",
       " 71.33317221633916,\n",
       " inf,\n",
       " 68.12685161280187,\n",
       " inf,\n",
       " 64.9269404476021,\n",
       " inf,\n",
       " 61.733425908238004,\n",
       " inf,\n",
       " 58.546295207816925,\n",
       " inf,\n",
       " 55.36553558500964,\n",
       " inf,\n",
       " 52.191134303999235,\n",
       " inf,\n",
       " 49.023078654425156,\n",
       " inf,\n",
       " 45.86135595133559,\n",
       " inf,\n",
       " 42.70595353513831,\n",
       " inf,\n",
       " 39.55685877154686,\n",
       " inf,\n",
       " 36.41405905153084,\n",
       " inf,\n",
       " 33.277541791265996,\n",
       " inf,\n",
       " 30.147294432081683,\n",
       " inf,\n",
       " 27.023304440417107,\n",
       " inf,\n",
       " 23.90555930776078,\n",
       " inf,\n",
       " 20.79404655061498,\n",
       " inf,\n",
       " 17.688753710423857,\n",
       " inf,\n",
       " 14.589668353551685,\n",
       " inf,\n",
       " 11.496778075149725,\n",
       " inf,\n",
       " 8.41007248247545,\n",
       " inf,\n",
       " 5.330534067555295,\n",
       " inf,\n",
       " 2.6923609239298782,\n",
       " inf,\n",
       " 82.79633476394504,\n",
       " inf,\n",
       " 79.567099325119,\n",
       " inf,\n",
       " 76.3443191310122,\n",
       " inf,\n",
       " 73.12798127756675,\n",
       " inf,\n",
       " 69.9180728865069,\n",
       " inf,\n",
       " 66.71458110530914,\n",
       " inf,\n",
       " 63.517493107148624,\n",
       " inf,\n",
       " 60.326796090823585,\n",
       " inf,\n",
       " 57.14247728073838,\n",
       " inf,\n",
       " 53.96452392682856,\n",
       " inf,\n",
       " 50.79292330451992,\n",
       " inf,\n",
       " 47.627662714672745,\n",
       " inf,\n",
       " 44.46872948353851,\n",
       " inf,\n",
       " 41.31611096269777,\n",
       " inf,\n",
       " 38.16979452901804,\n",
       " inf,\n",
       " 35.0297675846027,\n",
       " inf,\n",
       " 31.896017556735384,\n",
       " inf,\n",
       " 28.768531897833693,\n",
       " inf,\n",
       " 25.647298085396354,\n",
       " inf,\n",
       " 22.532303621962228,\n",
       " inf,\n",
       " 19.423536035045856,\n",
       " inf,\n",
       " 16.32098287709647,\n",
       " inf,\n",
       " 13.224631725579446,\n",
       " inf,\n",
       " 10.13447025171002,\n",
       " inf,\n",
       " 7.050520824671098,\n",
       " inf,\n",
       " 3.989931514529456,\n",
       " inf,\n",
       " 7.478738466273517,\n",
       " inf,\n",
       " 4.40743078933679,\n",
       " inf,\n",
       " 4.187249881171412,\n",
       " inf,\n",
       " 5.57711669364026,\n",
       " inf,\n",
       " 2.8147958296762985,\n",
       " inf,\n",
       " 73.75980120669345,\n",
       " inf,\n",
       " 70.54862983784857,\n",
       " inf,\n",
       " 67.34387760318523,\n",
       " inf,\n",
       " 64.14553167082671,\n",
       " inf,\n",
       " 60.95357923455184,\n",
       " inf,\n",
       " 57.76800751373803,\n",
       " inf,\n",
       " 54.588803753315474,\n",
       " inf,\n",
       " 51.41595522370208,\n",
       " inf,\n",
       " 48.24944922077305,\n",
       " inf,\n",
       " 45.08927306579268,\n",
       " inf,\n",
       " 41.93541410537629,\n",
       " inf,\n",
       " 38.78785971142276,\n",
       " inf,\n",
       " 35.646597281089086,\n",
       " inf,\n",
       " 32.511614236716554,\n",
       " inf,\n",
       " 29.382898025789295,\n",
       " inf,\n",
       " 26.260436120888,\n",
       " inf,\n",
       " 23.144216019635767,\n",
       " inf,\n",
       " 20.03422524464242,\n",
       " inf,\n",
       " 16.93045134346695,\n",
       " inf,\n",
       " 13.832881888591494,\n",
       " inf,\n",
       " 10.741504500148743,\n",
       " inf,\n",
       " 7.656318309622482,\n",
       " inf,\n",
       " 4.583031484982701,\n",
       " inf,\n",
       " 3.744478194217466,\n",
       " inf,\n",
       " 12.832511141723629,\n",
       " inf,\n",
       " 9.743133650611918,\n",
       " inf,\n",
       " 6.6600195090731305,\n",
       " inf,\n",
       " 3.62591771520957,\n",
       " inf,\n",
       " 16.270277350815302,\n",
       " inf,\n",
       " 13.174027591556381,\n",
       " inf,\n",
       " 10.083967327296898,\n",
       " inf,\n",
       " 7.00012915416842,\n",
       " inf,\n",
       " 3.9446620734363367,\n",
       " inf,\n",
       " 9.279743662095308,\n",
       " inf,\n",
       " 6.197698347351178,\n",
       " inf,\n",
       " 3.2314975160584196,\n",
       " inf,\n",
       " 36.441092627742464,\n",
       " inf,\n",
       " 33.30452140469508,\n",
       " inf,\n",
       " 30.174220189649382,\n",
       " inf,\n",
       " 27.050176448846265,\n",
       " inf,\n",
       " 23.932377673571718,\n",
       " inf,\n",
       " 20.820811380124564,\n",
       " inf,\n",
       " 17.71546510975354,\n",
       " inf,\n",
       " 14.616326428620795,\n",
       " inf,\n",
       " 11.523382932938304,\n",
       " inf,\n",
       " 8.436624863147774,\n",
       " inf,\n",
       " 5.3573502864658105,\n",
       " inf,\n",
       " 2.843879220725587,\n",
       " inf,\n",
       " 84.97268902073428,\n",
       " inf,\n",
       " 81.7391031213201,\n",
       " inf,\n",
       " 78.51198116231407,\n",
       " inf,\n",
       " 75.2913102222832,\n",
       " inf,\n",
       " 72.0770774056188,\n",
       " inf,\n",
       " 68.86926984249624,\n",
       " inf,\n",
       " 65.66787468881921,\n",
       " inf,\n",
       " 62.472879126160144,\n",
       " inf,\n",
       " 59.28427036171688,\n",
       " inf,\n",
       " 56.10203562826672,\n",
       " inf,\n",
       " 52.92616218410389,\n",
       " inf,\n",
       " 49.756637312991565,\n",
       " inf,\n",
       " 46.593448324118455,\n",
       " inf,\n",
       " 43.43658255203529,\n",
       " inf,\n",
       " 40.286027356617396,\n",
       " inf,\n",
       " 37.14177012300877,\n",
       " inf,\n",
       " 34.00379826156377,\n",
       " inf,\n",
       " 30.872099207807935,\n",
       " inf,\n",
       " 27.746660422382153,\n",
       " inf,\n",
       " 24.62746939099681,\n",
       " inf,\n",
       " 21.51451362437497,\n",
       " inf,\n",
       " 18.40778065820646,\n",
       " inf,\n",
       " 15.30725805310152,\n",
       " inf,\n",
       " 12.21293339597253,\n",
       " inf,\n",
       " 9.12479502250604,\n",
       " inf,\n",
       " 6.04319380854562,\n",
       " inf,\n",
       " 3.1380183305601568,\n",
       " inf,\n",
       " 51.218249773423054,\n",
       " inf,\n",
       " 48.052139029050664,\n",
       " inf,\n",
       " 44.89235734196321,\n",
       " inf,\n",
       " 41.73889206036062,\n",
       " inf,\n",
       " 38.59173055773622,\n",
       " inf,\n",
       " 35.450860232817604,\n",
       " inf,\n",
       " 32.316268509524065,\n",
       " inf,\n",
       " 29.187942836918815,\n",
       " inf,\n",
       " 26.06587068915239,\n",
       " inf,\n",
       " 22.950039565410993,\n",
       " inf,\n",
       " 19.8404369898784,\n",
       " inf,\n",
       " 16.73705051167001,\n",
       " inf,\n",
       " 13.639867704879922,\n",
       " inf,\n",
       " 10.548876211892251,\n",
       " inf,\n",
       " 7.464085581758355,\n",
       " inf,\n",
       " 4.396347237106888,\n",
       " inf,\n",
       " 5.518996744743295,\n",
       " inf,\n",
       " 2.9287092001802915,\n",
       " inf,\n",
       " 84.53369812653872,\n",
       " inf,\n",
       " 81.30098979446889,\n",
       " inf,\n",
       " 78.07474364825961,\n",
       " inf,\n",
       " 74.85494676998722,\n",
       " inf,\n",
       " 71.64158626754966,\n",
       " inf,\n",
       " 68.43464927461756,\n",
       " inf,\n",
       " 65.23412295058333,\n",
       " inf,\n",
       " 62.03999448050105,\n",
       " inf,\n",
       " 58.85225107505195,\n",
       " inf,\n",
       " 55.670879970473194,\n",
       " inf,\n",
       " 52.495868428526265,\n",
       " inf,\n",
       " 49.327203736425695,\n",
       " inf,\n",
       " 46.16487320680971,\n",
       " inf,\n",
       " 43.008864177673146,\n",
       " inf,\n",
       " 39.8591640123218,\n",
       " inf,\n",
       " 36.71576009932331,\n",
       " inf,\n",
       " 33.57863985245779,\n",
       " inf,\n",
       " 30.44779071066507,\n",
       " inf,\n",
       " 27.32320013798906,\n",
       " inf,\n",
       " 24.20485562353988,\n",
       " inf,\n",
       " 21.092744681432244,\n",
       " inf,\n",
       " 17.98685485074628,\n",
       " inf,\n",
       " 14.887173695466489,\n",
       " inf,\n",
       " 11.793688808355576,\n",
       " inf,\n",
       " 8.706389775877614,\n",
       " inf,\n",
       " 5.62624990646088,\n",
       " inf,\n",
       " 2.9837992191025426,\n",
       " inf,\n",
       " 82.92999432795226,\n",
       " inf]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Try different learning rates and compare the results. How does the learning rate influence the convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Try different regularization parameter values and compare the model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Compare zero initialization and random initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you need to implement weighted K-Neighbors Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that training a KNN classifier is simply memorizing a training sample. \n",
    "\n",
    "The process of applying a classifier for one object is to find the distances from it to all objects in the training data, then select the k nearest objects (neighbors) and return the most common class among these objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also give the nearest neighbors weights in accordance with the distance of the object to them. In the simplest case (as in your assignment), you can set the weights inversely proportional to that distance. \n",
    "\n",
    "$$w_{i} = \\frac{1}{d_{i} + eps},$$\n",
    "\n",
    "where $d_{i}$ is the distance between object and i-th nearest neighbor and $eps$ is the small value to prevent division by zero.\n",
    "\n",
    "In case of 'uniform' weights, all k nearest neighbors are equivalent (have equal weight, for example $w_{i} = 1, \\forall i \\in(1,k)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the probability of classes, it is necessary to normalize the weights of each class, dividing them by the sum:\n",
    "\n",
    "$$p_{i} = \\frac{w_{i}}{\\sum_{j=1}^{c}w_{j}},$$\n",
    "\n",
    "where $p_i$ is probability of i-th class and $c$ is the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2 points)** Implement the algorithm and use it to classify the digits. By implementing this algorithm, you will be able to classify numbers not only into \"even\" or \"odd\", but into their real representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomKNeighborsClassifier:\n",
    "    _estimator_type = \"classifier\"\n",
    "    \n",
    "    def __init__(self, n_neighbors=5, weights='uniform', eps=1e-9):\n",
    "        \"\"\"K-Nearest Neighbors classifier.\n",
    "        \n",
    "        Args:\n",
    "            n_neighbors: int, default=5\n",
    "                Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
    "            weights : {'uniform', 'distance'} or callable, default='uniform'\n",
    "                Weight function used in prediction.  Possible values:\n",
    "                - 'uniform' : uniform weights.  All points in each neighborhood\n",
    "                  are weighted equally.\n",
    "                - 'distance' : weight points by the inverse of their distance.\n",
    "                  in this case, closer neighbors of a query point will have a\n",
    "                  greater influence than neighbors which are further away.\n",
    "            eps : float, default=1e-5\n",
    "                Epsilon to prevent division by 0 \n",
    "        \"\"\"\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "        self.eps = eps\n",
    "        \n",
    "    \n",
    "    def get_pairwise_distances(self, X, Y):\n",
    "        \"\"\"\n",
    "        Returnes matrix of the pairwise distances between the rows from both X and Y.\n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "            Y: numpy array of shape (k_samples, n_features)\n",
    "        Returns:\n",
    "            P: numpy array of shape (n_samples, k_samples)\n",
    "                Matrix in which (i, j) value is the distance \n",
    "                between i'th row from the X and j'th row from the Y.\n",
    "        \"\"\"\n",
    "        # <your code>\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def get_class_weights(self, y, weights):\n",
    "        \"\"\"\n",
    "        Returns a vector with sum of weights for each class \n",
    "        Args:\n",
    "            y: numpy array of shape (n_samles,)\n",
    "            weights: numpy array of shape (n_samples,)\n",
    "                The weights of the corresponding points of y.\n",
    "        Returns:\n",
    "            p: numpy array of shape (n_classes)\n",
    "                Array where the value at the i-th position \n",
    "                corresponds to the weight of the i-th class.\n",
    "        \"\"\"\n",
    "        # <your code>\n",
    "        pass\n",
    "            \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Target vector.        \n",
    "        \"\"\"\n",
    "        self.points = X\n",
    "        self.y = y\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict positive class probabilities.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples, n_classes)\n",
    "                Vector containing positive class probabilities.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'points'):\n",
    "            P = self.get_pairwise_distances(X, self.points)\n",
    "            \n",
    "            weights_of_points = np.ones(P.shape)\n",
    "            if self.weights == 'distance':\n",
    "                weights_of_points = 'your code'\n",
    "                \n",
    "            # <your code>\n",
    "            pass\n",
    "        \n",
    "        else: \n",
    "            raise NotFittedError(\"CustomKNeighborsClassifier instance is not fitted yet\")\n",
    "            \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing predicted class labels.\n",
    "        \"\"\"\n",
    "        # <your code>\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomKNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(model.get_pairwise_distances(np.array([[0  , 1]  , [1, 1]]), \n",
    "                                                np.array([[0.5, 0.5], [1, 0]])),\n",
    "                   np.array([[0.70710678, 1.41421356],\n",
    "                             [0.70710678, 1.        ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_ = ['one', 'two', 'three']\n",
    "assert np.allclose(model.get_class_weights(np.array(['one', 'one', 'three', 'two']), np.array([1, 1, 0, 4])), \n",
    "                   np.array([2,4,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "_, axes = plt.subplots(nrows=3, ncols=7, figsize=(10, 5))\n",
    "for ax, image, label in zip(axes.flatten(), X, y):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image.reshape((8, 8)), cmap=plt.cm.gray_r if label % 2 else plt.cm.afmhot_r)\n",
    "    ax.set_title(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "knn.fit(X_train, list(map(str, y_train)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(model.predict_proba(X_test), knn.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, test_acc = fit_evaluate(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_acc == 1\n",
    "assert test_acc > 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Take a look at the confusion matrix and tell what numbers the model confuses and why this happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Try different n_neighbors parameters and compare the output probabilities of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Compare both 'uniform' and 'distance' weights and share your thoughts in what situations which parameter can be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Suggest another distance measurement function that could improve the quality of the classification for this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Suggest different task and distance function that you think would be suitable for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Synthetic Titanic Survival Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Read the description here: https://www.kaggle.com/c/tabular-playground-series-apr-2021/data. Download the dataset and place it in the *data/titanic/* folder in your working directory.\n",
    "You will use train.csv for model training and validation. The test set is used for model testing: once the model is trained, you can predict whether a passenger survived or not for each passenger in the test set, and submit the predictions: https://www.kaggle.com/c/tabular-playground-series-apr-2021/overview/evaluation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(PATH, 'titanic', 'train.csv')).set_index('PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** How many females and males are there in the dataset? What about the survived passengers? Is there any relationship between the gender and the survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Plot age distribution of the passengers. What is the average and the median age of survived and deceased passengers? Do age distributions differ for survived and deceased passengers? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 point)** Explore \"passenger class\" and \"embarked\" features. What class was \"the safest\"? Is there any relationship between the embarkation port and the survival? Provide the corresponding visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Find the percentage of missing values for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the ways to handle these missing values for modelling and write your answer below. Which methods would you suggest? What are their advantages and disadvantages?\n",
    "\n",
    "< your thoughts >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.5 points)** Prepare the features and train two models (KNN and Logistic Regression) to predict the survival. Compare the results. Use accuracy as a metric. Don't forget about cross-validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 + X points)** Try more feature engineering and hyperparameter tuning to improve the results. You may use either KNN or Logistic Regression (or both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the best model, load the test set and make the predictions. Submit them to kaggle and see the results :)\n",
    "\n",
    "**Note**. X points will depend on your kaggle public leaderboard score.\n",
    "$$ f(score) = 1.0, \\ \\ 0.79 \\leq score < 0.80,$$\n",
    "$$ f(score) = 2.5, \\ \\ 0.80 \\leq score < 0.81,$$ \n",
    "$$ f(score) = 4.0, \\ \\ 0.81 \\leq score $$ \n",
    "Your code should generate the output submitted to kaggle. Fix random seeds to make the results reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
